#!/usr/bin/env python3
"""
Example Route Discovery Script
Algorithmically discovers routes with infinite loop prevention
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin, urlparse, parse_qs, urlencode, urlunparse
import time
import sys


def normalize_url(url):
    """Normalize URL to prevent duplicate visits."""
    parsed = urlparse(url)

    # Remove fragment
    parsed = parsed._replace(fragment='')

    # Sort query parameters for consistency
    if parsed.query:
        params = parse_qs(parsed.query, keep_blank_values=True)
        sorted_params = sorted(params.items())
        query = urlencode(sorted_params, doseq=True)
        parsed = parsed._replace(query=query)

    # Rebuild URL
    return urlunparse(parsed)


def is_same_domain(url, base_url):
    """Check if URL belongs to the same domain as base_url."""
    return urlparse(url).netloc == urlparse(base_url).netloc or urlparse(url).netloc == ''


def discover_routes(base_url, max_depth=3, timeout=60):
    """
    Discover routes in a web application with infinite loop prevention.

    Args:
        base_url: Starting URL (e.g., http://localhost:3000)
        max_depth: Maximum crawl depth to prevent infinite recursion
        timeout: Overall timeout in seconds

    Returns:
        Dict with discovered routes and endpoints
    """
    start_time = time.time()

    # Data structures for loop prevention
    visited_urls = set()
    to_visit = [(base_url, 0)]  # (url, depth)

    # Results
    discovered = {
        'endpoints': [],
        'pages': [],
        'api_routes': [],
        'forms': [],
        'input_vectors': [],
        'technology_detected': '',
        'security_headers': {},
        'cookies': []
    }

    endpoints_seen = set()

    print(f"[*] Starting route discovery: {base_url}", file=sys.stderr)
    print(f"[*] Max depth: {max_depth}, Timeout: {timeout}s", file=sys.stderr)

    while to_visit and (time.time() - start_time) < timeout:
        url, depth = to_visit.pop(0)

        # Normalize URL to prevent duplicates
        normalized = normalize_url(url)

        # Skip if already visited
        if normalized in visited_urls:
            continue

        # Skip if max depth exceeded
        if depth > max_depth:
            print(f"[*] Skipping {url} (depth {depth} > max {max_depth})", file=sys.stderr)
            continue

        # Skip external domains
        if not is_same_domain(url, base_url):
            print(f"[*] Skipping external URL: {url}", file=sys.stderr)
            continue

        visited_urls.add(normalized)

        try:
            print(f"[*] Visiting: {url} (depth {depth})", file=sys.stderr)

            # Make request with timeout
            response = requests.get(url, timeout=5, allow_redirects=True)

            # Extract path from URL
            path = urlparse(url).path or '/'

            # Categorize the route
            if path.startswith('/api/'):
                if path not in [e['path'] for e in discovered['api_routes']]:
                    discovered['api_routes'].append(path)
            elif path not in discovered['pages']:
                discovered['pages'].append(path)

            # Extract cookies
            for cookie in response.cookies:
                cookie_info = {
                    'name': cookie.name,
                    'domain': cookie.domain,
                    'path': cookie.path,
                    'secure': cookie.secure,
                    'httponly': cookie.has_nonstandard_attr('HttpOnly')
                }
                if cookie_info not in discovered['cookies']:
                    discovered['cookies'].append(cookie_info)

            # Detect technology from headers
            if 'X-Powered-By' in response.headers:
                discovered['technology_detected'] = response.headers['X-Powered-By']
            elif 'Server' in response.headers:
                discovered['technology_detected'] = response.headers['Server']

            # Check security headers
            security_headers = [
                'X-Frame-Options', 'X-Content-Type-Options', 'X-XSS-Protection',
                'Strict-Transport-Security', 'Content-Security-Policy'
            ]
            for header in security_headers:
                if header in response.headers:
                    discovered['security_headers'][header] = response.headers[header]
                else:
                    discovered['security_headers'][header] = 'Missing'

            # Parse HTML content
            if 'text/html' in response.headers.get('Content-Type', ''):
                soup = BeautifulSoup(response.text, 'html.parser')

                # Find all links
                for tag in soup.find_all(['a', 'link']):
                    href = tag.get('href')
                    if href:
                        absolute_url = urljoin(url, href)

                        # Only queue if same domain
                        if is_same_domain(absolute_url, base_url):
                            to_visit.append((absolute_url, depth + 1))

                # Find forms
                for form in soup.find_all('form'):
                    action = form.get('action', '')
                    method = form.get('method', 'GET').upper()
                    absolute_action = urljoin(url, action) if action else url

                    fields = []
                    for input_tag in form.find_all(['input', 'textarea', 'select']):
                        field_name = input_tag.get('name')
                        field_type = input_tag.get('type', 'text')
                        if field_name:
                            fields.append({
                                'name': field_name,
                                'type': field_type
                            })

                    form_info = {
                        'action': absolute_action,
                        'method': method,
                        'fields': fields,
                        'found_on': path
                    }

                    if form_info not in discovered['forms']:
                        discovered['forms'].append(form_info)

                # Extract API endpoints from JavaScript
                scripts = soup.find_all('script')
                for script in scripts:
                    if script.string:
                        # Look for fetch() calls
                        fetch_patterns = [
                            r"fetch\(['\"]([^'\"]+)['\"]",
                            r"axios\.(?:get|post|put|delete)\(['\"]([^'\"]+)['\"]",
                            r"\.ajax\({[^}]*url:\s*['\"]([^'\"]+)['\"]",
                        ]

                        for pattern in fetch_patterns:
                            matches = re.findall(pattern, script.string)
                            for match in matches:
                                api_url = urljoin(url, match)
                                if is_same_domain(api_url, base_url):
                                    api_path = urlparse(api_url).path
                                    if api_path not in [e['path'] for e in discovered['api_routes']]:
                                        discovered['api_routes'].append(api_path)

            # Try to detect supported HTTP methods
            try:
                options_response = requests.options(url, timeout=3)
                allowed_methods = options_response.headers.get('Allow', '').split(',')
                allowed_methods = [m.strip() for m in allowed_methods if m.strip()]
            except:
                # Assume common methods if OPTIONS fails
                allowed_methods = ['GET']

            # Extract query parameters
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            param_names = list(params.keys())

            # Create endpoint entry
            endpoint_key = f"{path}:{','.join(sorted(allowed_methods))}"
            if endpoint_key not in endpoints_seen:
                endpoints_seen.add(endpoint_key)

                endpoint = {
                    'path': path,
                    'full_url': url,
                    'methods': allowed_methods,
                    'parameters': param_names,
                    'status_code': response.status_code,
                    'depth': depth,
                    'content_type': response.headers.get('Content-Type', 'unknown')
                }

                discovered['endpoints'].append(endpoint)

                # Add input vectors
                for param in param_names:
                    input_vector = {
                        'location': f"{path}?{param}=",
                        'type': 'query_parameter',
                        'parameter': param,
                        'endpoint': path
                    }
                    if input_vector not in discovered['input_vectors']:
                        discovered['input_vectors'].append(input_vector)

        except requests.Timeout:
            print(f"[!] Timeout visiting {url}", file=sys.stderr)
        except requests.RequestException as e:
            print(f"[!] Error visiting {url}: {e}", file=sys.stderr)
        except Exception as e:
            print(f"[!] Unexpected error visiting {url}: {e}", file=sys.stderr)

    # Summary
    elapsed = time.time() - start_time
    print(f"\n[*] Discovery complete in {elapsed:.2f}s", file=sys.stderr)
    print(f"[*] Visited {len(visited_urls)} unique URLs", file=sys.stderr)
    print(f"[*] Found {len(discovered['endpoints'])} endpoints", file=sys.stderr)
    print(f"[*] Found {len(discovered['api_routes'])} API routes", file=sys.stderr)
    print(f"[*] Found {len(discovered['pages'])} pages", file=sys.stderr)
    print(f"[*] Found {len(discovered['forms'])} forms", file=sys.stderr)

    return discovered


if __name__ == "__main__":
    # Default target
    target = sys.argv[1] if len(sys.argv) > 1 else "http://localhost:3000"

    result = discover_routes(target, max_depth=3, timeout=60)

    # Output JSON
    print(json.dumps(result, indent=2))
