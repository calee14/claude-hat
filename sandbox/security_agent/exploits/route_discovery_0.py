#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin, urlparse, parse_qs, urlencode
import time
import sys

def discover_routes(base_url, max_depth=3, timeout=60):
    start_time = time.time()
    visited = set()
    endpoints = []
    pages = []
    api_routes = []
    input_vectors = []
    technology_detected = "Unknown"
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    base_domain = urlparse(base_url).netloc
    
    def normalize_url(url):
        parsed = urlparse(url)
        query_dict = parse_qs(parsed.query)
        normalized_query = urlencode(sorted(query_dict.items()))
        return f"{parsed.scheme}://{parsed.netloc}{parsed.path}{'?' + normalized_query if normalized_query else ''}"
    
    def is_valid_url(url):
        parsed = urlparse(url)
        return parsed.netloc == base_domain and parsed.scheme in ['http', 'https']
    
    def extract_js_endpoints(content):
        endpoints = []
        patterns = [
            r'["\']\/api\/[^"\']*["\']',
            r'fetch\(["\']([^"\']*)["\']',
            r'axios\.[get|post|put|delete]*\(["\']([^"\']*)["\']',
            r'["\']\/[_a-zA-Z][^"\']*\.json["\']',
            r'endpoint["\']?\s*:\s*["\']([^"\']*)["\']',
            r'url["\']?\s*:\s*["\']([^"\']*)["\']',
            r'["\']\/graphql[^"\']*["\']',
            r'["\']\/v\d+\/[^"\']*["\']'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match[0] else match[1]
                match = match.strip('\'"')
                if match and not match.startswith('http'):
                    endpoints.append(match)
        
        return endpoints
    
    def detect_technology(content, headers):
        tech = "Unknown"
        
        if 'x-powered-by' in headers:
            if 'Next.js' in headers['x-powered-by']:
                tech = "Next.js"
            elif 'Express' in headers['x-powered-by']:
                tech = "Express.js"
        
        if '_next' in content or '__NEXT_DATA__' in content:
            tech = "Next.js"
        elif 'nuxt' in content.lower():
            tech = "Nuxt.js"
        elif 'react' in content.lower():
            tech = "React"
        elif 'vue' in content.lower():
            tech = "Vue.js"
        elif 'angular' in content.lower():
            tech = "Angular"
        
        return tech
    
    def extract_parameters(url, soup, forms):
        params = set()
        
        # From URL query parameters
        parsed = urlparse(url)
        if parsed.query:
            params.update(parse_qs(parsed.query).keys())
        
        # From form inputs
        for form in forms:
            inputs = form.find_all(['input', 'select', 'textarea'])
            for inp in inputs:
                name = inp.get('name')
                if name:
                    params.add(name)
        
        # From data attributes and common parameter names in content
        if soup:
            for elem in soup.find_all(attrs={"data-param": True}):
                params.add(elem.get('data-param'))
            
            # Common parameter patterns in text
            content = soup.get_text()
            param_patterns = [r'[?&](\w+)=', r'"(\w+)"\s*:', r"'(\w+)'\s*:"]
            for pattern in param_patterns:
                matches = re.findall(pattern, content)
                params.update(matches)
        
        return list(params)
    
    def crawl(url, depth=0):
        nonlocal technology_detected
        
        if time.time() - start_time > timeout:
            return
        
        if depth > max_depth:
            return
        
        normalized = normalize_url(url)
        if normalized in visited:
            return
        
        visited.add(normalized)
        
        try:
            response = session.get(url, timeout=5, allow_redirects=True)
            headers = dict(response.headers)
            
            if technology_detected == "Unknown":
                technology_detected = detect_technology(response.text, headers)
            
            parsed_url = urlparse(url)
            path = parsed_url.path
            
            # Categorize the endpoint
            is_api = any(api_indicator in path.lower() for api_indicator in ['/api/', '/graphql', '/v1/', '/v2/', '/v3/'])
            
            soup = None
            forms = []
            links = []
            
            if 'text/html' in response.headers.get('content-type', ''):
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Extract forms
                forms = soup.find_all('form')
                for form in forms:
                    action = form.get('action', '')
                    if action:
                        full_action = urljoin(url, action)
                        if is_valid_url(full_action):
                            method = form.get('method', 'GET').upper()
                            input_vectors.append({
                                'url': full_action,
                                'method': method,
                                'type': 'form',
                                'inputs': [inp.get('name') for inp in form.find_all('input') if inp.get('name')]
                            })
                
                # Extract links
                for link in soup.find_all(['a', 'link'], href=True):
                    href = link['href']
                    full_url = urljoin(url, href)
                    if is_valid_url(full_url):
                        links.append(full_url)
                
                # Extract script and img sources
                for elem in soup.find_all(['script', 'img'], src=True):
                    src = elem['src']
                    full_url = urljoin(url, src)
                    if is_valid_url(full_url):
                        links.append(full_url)
                
                # Extract JS endpoints
                js_endpoints = extract_js_endpoints(response.text)
                for endpoint in js_endpoints:
                    full_endpoint = urljoin(url, endpoint)
                    if is_valid_url(full_endpoint):
                        links.append(full_endpoint)
            
            # Extract parameters
            parameters = extract_parameters(url, soup, forms)
            
            # Determine HTTP methods
            methods = ['GET']
            if forms:
                for form in forms:
                    method = form.get('method', 'GET').upper()
                    if method not in methods:
                        methods.append(method)
            
            if is_api:
                methods.extend(['POST', 'PUT', 'DELETE'])
                methods = list(set(methods))
            
            # Create endpoint entry
            endpoint_entry = {
                'url': path,
                'methods': methods,
                'parameters': parameters,
                'forms': len(forms),
                'depth': depth,
                'status_code': response.status_code
            }
            
            endpoints.append(endpoint_entry)
            
            if is_api:
                api_routes.append(path)
            else:
                pages.append(path)
            
            # Continue crawling links
            for link in links:
                if time.time() - start_time > timeout:
                    break
                crawl(link, depth + 1)
            
        except requests.exceptions.RequestException:
            pass  # Ignore request errors
        except Exception:
            pass  # Ignore parsing errors
    
    # Common paths to check
    common_paths = [
        '/api', '/api/users', '/api/auth', '/api/login', '/api/admin',
        '/admin', '/admin/login', '/admin/users', '/admin/dashboard',
        '/login', '/logout', '/register', '/profile', '/dashboard',
        '/health', '/status', '/ping', '/version', '/docs', '/swagger',
        '/graphql', '/api/graphql', '/_next/static', '/static', '/assets',
        '/robots.txt', '/sitemap.xml', '/favicon.ico', '/.env', '/config',
        '/api/v1', '/api/v2', '/v1', '/v2', '/rest', '/api/rest'
    ]
    
    # Start crawling from base URL
    crawl(base_url, 0)
    
    # Check common paths
    for path in common_paths:
        if time.time() - start_time > timeout:
            break
        full_url = urljoin(base_url, path)
        if normalize_url(full_url) not in visited:
            crawl(full_url, 1)
    
    # Remove duplicates and sort
    pages = sorted(list(set(pages)))
    api_routes = sorted(list(set(api_routes)))
    
    # Deduplicate endpoints by URL
    seen_urls = set()
    unique_endpoints = []
    for endpoint in endpoints:
        if endpoint['url'] not in seen_urls:
            seen_urls.add(endpoint['url'])
            unique_endpoints.append(endpoint)
    
    return {
        'endpoints': unique_endpoints,
        'pages': pages,
        'api_routes': api_routes,
        'input_vectors': input_vectors,
        'technology_detected': technology_detected,
        'total_discovered': len(unique_endpoints),
        'scan_duration': round(time.time() - start_time, 2)
    }

if __name__ == "__main__":
    result = discover_routes("http://localhost:3000")
    print(json.dumps(result, indent=2))